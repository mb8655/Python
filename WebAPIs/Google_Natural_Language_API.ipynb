{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP/sQY8XwZc9wcLh8l0JGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mb8655/Python/blob/main/Google_Natural_Language_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interacting with the Google Cloud Natural Language API\n",
        "Another useful API, especially when dealing with text, is the Google Cloud Natural Language API, which offers a variety of text analysis functionalities, such as sentiment analysis, entity extraction, keyword extraction, etc.\n",
        "\n",
        "We will give a couple of examples below, to understand how we can take an unstructured piece of text (either the text alone, or a URL with text), and extract some \"semi-structured\" representation of its content.\n",
        "\n",
        "/analyzeSentiment call\n",
        "We will first start with the /analyzeSentiment API call (documentation & also here) which takes as input a piece of text, and returns an analysis across various dimensions.\n",
        "\n",
        "The call below gets as input a \"text\" variable, and returns back the sentiment of the text."
      ],
      "metadata": {
        "id": "Ew5tCQ6O9FQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "OIxB8h-Y9Az7",
        "outputId": "b1e4ecbe-37f6-4d02-c0af-2718e18c6d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/api_keys.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-eb6487e7afb3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/api_keys.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mapi_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/api_keys.json'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import json\n",
        "with open('/content/drive/My Drive/api_keys.json') as f:\n",
        "    api_keys = json.load(f)\n",
        "\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(text, api_key):\n",
        "    url = f\"https://language.googleapis.com/v1/documents:analyzeSentiment?key={api_key}\"\n",
        "\n",
        "    data = {\n",
        "        \"document\": {\n",
        "            \"type\": \"PLAIN_TEXT\",\n",
        "            \"content\": text\n",
        "        },\n",
        "        \"encodingType\": \"UTF8\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=data)\n",
        "\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "VP2ht3M_9X_W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will analyze the text below using the IBM Watson API\n",
        "\n",
        "text = '''\n",
        "I got their Egg & Cheese sandwich on a Whole Wheat Everything Bagel.\n",
        "First off, I loved loved loved the texture of the bagel itself.\n",
        "It was very chewy yet soft, which is a top feature for a NY style bagel.\n",
        "However, I thought there could've been more seasoning on top of\n",
        "the bagel as I found the bagel itself to be a bit bland.\n",
        "\n",
        "Speaking of bland, I thought the egg and cheese filling were also quite bland.\n",
        "This was definitely lacking salt and pepper in the eggs and the cheese didn't\n",
        "really add too much flavor either, which was really disappointing!\n",
        "My mom also had the same complaint with her bagel sandwich\n",
        "(she had the egg sandwich on a blueberry bagel) so I definitely wasn't\n",
        "the only one.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Dsb-YpAT-wE3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = analyze_sentiment(text, api_keys['google_nlp_api_key'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "2d88F7W3-yIW",
        "outputId": "3fd0f541-fe4e-4f95-ceff-4d19d360cc4a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'api_keys' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a7fdbda0dd5c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'google_nlp_api_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'api_keys' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's try to understand the structure of the answer. First, we check the high-level keys.\n",
        "\n",
        "data.keys()\n"
      ],
      "metadata": {
        "id": "oxMW0Pq4-3Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dict_keys(['documentSentiment', 'language', 'sentences'])\n",
        "Now, let's check the content of these keys:"
      ],
      "metadata": {
        "id": "fkoU2qRb-225"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['documentSentiment']\n"
      ],
      "metadata": {
        "id": "-K0tpVNF_B0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'magnitude': 5.1, 'score': -0.1}\n"
      ],
      "metadata": {
        "id": "sjAkfGBe_CvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's go deeper into the 'sentence'\n",
        "data['sentences']"
      ],
      "metadata": {
        "id": "EfMOTvXu_GPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And a bit more\n",
        "print(f\"The sentiment in this text is {data['documentSentiment']['score']}\")"
      ],
      "metadata": {
        "id": "4zI-SNKU_Kj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entities call\n",
        "Full Documentation of the call\n",
        "\n",
        "This is a an API call that extracts entities from the text, and also the sentiment for each of these entities.\n",
        "\n",
        "In terms of natural language processing, we will examine a couple of capabilities of the API. First, you will see that there is the capability of \"normalizing\" each entity, so that two different ways of saying the same thing get mapped to the same entity. So for example, \"President Biden\" and \"Joe Biden\" get mapped to the same Knowledge Graph entity."
      ],
      "metadata": {
        "id": "I8MuMoGQ_RUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def extract_entities(url_to_analyze, api_key):\n",
        "    html_content = requests.get(url_to_analyze).text\n",
        "    url = f\"https://language.googleapis.com/v1/documents:analyzeEntities?key={api_key}\"\n",
        "\n",
        "    data = {\n",
        "        \"document\": {\n",
        "            \"type\": \"HTML\",\n",
        "            \"content\": html_content\n",
        "        },\n",
        "        \"encodingType\": \"UTF8\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=data)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "DGsFwT1__Vi0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_to_analyze = 'https://www.reuters.com/lifestyle/elvis-everything-everywhere-vie-oscar-nods-tuesday-2023-01-24/'\n",
        "\n",
        "data = extract_entities(url_to_analyze, api_keys['google_nlp_api_key'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "uI5bVcyd_awU",
        "outputId": "21bb0369-bf16-4a24-a0ea-a06bfc70198a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'api_keys' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-db99838033de>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl_to_analyze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.reuters.com/lifestyle/elvis-everything-everywhere-vie-oscar-nods-tuesday-2023-01-24/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_analyze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'google_nlp_api_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'api_keys' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see what we get back as top-level attributes\n",
        "data.keys()"
      ],
      "metadata": {
        "id": "qmAyWFQy_hEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let' see the entities list\n",
        "data['entities']"
      ],
      "metadata": {
        "id": "9jhTDbzd_i4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes as input the result\n",
        "# from the IBM Watson API and returns a list\n",
        "# of entities that are relevant (above threshold)\n",
        "# to the article\n",
        "def getEntities(data, threshold):\n",
        "    result = []\n",
        "    for entity in data[\"entities\"]:\n",
        "        relevance = float(entity['salience'])\n",
        "        if relevance > threshold:\n",
        "            result.append(entity['name'])\n",
        "    return result\n",
        "\n",
        "getEntities(data, 0.002)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "PEVtWNf9_z5I",
        "outputId": "10b59d0a-0e76-4c63-ac72-e75522baad82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f7b3136b78f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgetEntities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " classify content tool to the content type of a website. You can find the documentation here and here. https://cloud.google.com/natural-language/docs/categories  https://cloud.google.com/natural-language/docs/reference/rest/v1/ClassificationCategory"
      ],
      "metadata": {
        "id": "KTWSEO9w_RMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: use the classify content tool of the natural language to classify the content of a website\n",
        "\n",
        "def classify_content(text, api_key):\n",
        "    url = f\"https://language.googleapis.com/v1/documents:classifyText?key={api_key}\"\n",
        "\n",
        "    data = {\n",
        "        \"document\": {\n",
        "            \"type\": \"PLAIN_TEXT\",\n",
        "            \"content\": text\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=data)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "# Assuming you have the website content in the 'text' variable\n",
        "# You can use requests library to fetch the website content.\n",
        "# Example:\n",
        "# text = requests.get('https://www.example.com').text\n",
        "\n",
        "\n",
        "data = classify_content(text, api_keys['google_nlp_api_key'])\n",
        "\n",
        "\n",
        "# Now you can access the categories from the response\n",
        "if 'categories' in data:\n",
        "    for category in data['categories']:\n",
        "        print(f\"Category: {category['name']}, Confidence: {category['confidence']}\")\n",
        "else:\n",
        "    print(\"No categories found in the response.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sGpqtF4TAGcd",
        "outputId": "b2f02ad0-92b1-441e-a568-6096ec5c3a12"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'api_keys' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2a62dac51c3d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'google_nlp_api_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'api_keys' is not defined"
          ]
        }
      ]
    }
  ]
}